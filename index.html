<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Your Data Engineering Portfolio</title>
    <link rel="stylesheet" href="styles.css"> <!-- Link to external CSS file -->
</head>
<body>
    <header>
        <div class="container">
            <h2>About Me: Abhishek Teli, Data Engineer</h2>
        <p>Hello! I'm Abhishek Teli, a dedicated and results-driven Data Engineer with over three years of experience specializing in cloud-based data solutions and scalable data pipelines. My professional journey at esteemed organizations like EXLService and Barclays has equipped me with a profound understanding of data infrastructure, ETL processes, and cloud services.</p>
        <p><strong>Career Highlights:</strong></p>
        <ul>
          <li>Innovative Problem-Solving: At EXLService, I spearheaded the development of a multi-account AWS data pipeline, enhancing data ingestion and transformation efficiency by 40%. My role involved optimizing AWS Data Sync and Kinesis for robust data ingestion, employing AWS Lambda and EventBridge for streamlined pipeline automation, and ensuring data integrity with AWS Lambda and Glue PySpark scripts.</li>
          <li>Data Pipeline Mastery: During my tenure at Barclays, I designed and implemented a transformative data pipeline managing over 150 million records, significantly reducing manual effort and improving data processing time. My work with PySpark, Airflow DAGs, and AWS Glue facilitated the seamless migration of large data sets, improving runtime by 40% and contributing to a more scalable project infrastructure.</li>
          <li>Educational Foundation: I hold a Master's degree in Computer Science from Syracuse University, where I deepened my knowledge in critical areas such as Data Structures, Machine Learning Algorithms, Data Modeling, Data Warehousing, and Cloud Computing.</li>
          <li>Technical Proficiency: My expertise spans a wide range of tools and technologies, including ETL/ELT tools (Spark, Kafka, AWS Glue), Data Warehousing/Data Lake solutions (S3, Redshift, BigQuery), programming languages (Python, Java, Scala), and cloud platforms (AWS, GCP).</li>
          <li>Passion for Innovation: Beyond my technical roles, I thrive on tackling challenging problems, optimizing data workflows, and contributing to projects that leverage data for actionable insights. My recent project, a Real Estate Chat Bot, exemplifies my ability to integrate cutting-edge technologies like LangChain and ChatGPT to enhance user engagement and data-driven decision-making.</li>
        </ul>
        <p>Connect with Me:</p>
        <p>Whether you're interested in discussing potential collaborations, exploring innovative data solutions, or simply connecting, I'm always open to engaging with fellow professionals and enthusiasts in the field. Let's explore how we can drive data-forward initiatives together!</p>
        <p><a href="https://www.linkedin.com/in/abhishekteli">LinkedIn: abhishekteli</a> | <a href="https://github.com/abhishekteli">GitHub: abhishekteli</a></p>
        </div>
    </header>
    
    <section id="projects" class="info-section">
        <div class="container">
        <h2>Projects</h2>
        <p>Here are some of my projects hosted on GitHub. Please feel free to check them out:</p>
        <ul>
          <li><a href="https://github.com/abhishekteli/Real-Estate-Data-Pipeline-Batch" target="_blank">Real-Estate-Data-Pipeline-Batch</a> - A Batch Data pipeline, which uses Python to fetch Real Estate Data in JSON format from Zillow API. It then stages the JSON data in RawData files, post which PySpark script is triggered which transforms the data, applies structured schema and stores the data into CSV files and also loads the data into PostgreSQL Data Warehouse, the pipeline is triggered daily using Apache Airflow</li>
          <li><a href="https://github.com/abhishekteli/DocuMentor-RAG-Pipeline" target="_blank">DocuMentor-RAG-Pipeline</a> - A RAG Data Pipeline application, which is built using Lang Chain for loading and embedding the data using OpenAI Embeddings and stores the embedded vectors in ChromaDB. Users can provide Prompt to the application, which then triggers the pipeline which embeds the user prompt and provides the user with relevant chunk of information</li>
          <li><a href="https://github.com/abhishekteli/Real-Estate-Data-Pipeline-Streaming" target="_blank">Real-Estate-Data-Pipeline-Streaming</a> - A Real Time Data pipeline, which uses Python to fetch Real Estate Data in JSON format in real time from Zillow API. It then stages the JSON data in Kafka cluster, post which Spark Streaming application is reads teh kafka stream which transforms the data, applies structured schema and stores the data into CSV files and also loads the data into PostgreSQL Data Warehouse in real time.</li>
          <li><a href="https://github.com/abhishekteli/RealTime-ClickStream-DataAnalytics-Pipeline" target="_blank">RealTime-ClickStream-DataAnalytics-Pipeline</a> - This project is a real-time streaming data engineering project that captures clickstream data from a self-built e-commerce website and processes it in real-time using Apache Kafka, Apache Spark Streaming, and Apache Cassandra. The project consists of two Spark applications: one that cleans and processes the data in real-time and loads it into Cassandra, and another that identifies frequent visitors to the website and sends them personalized email offers. Streamlit is used to build the e-commerce website.</li>  
          <li><a href="https://github.com/abhishekteli/COVID-19-Data-Analytics-Pipeline" target="_blank">COVID-19-Data-Analytics-Pipeline</a> - This project is aimed at building a data pipeline for Covid data analytics using various technologies such as Apache Airflow, HDFS, PySpark, Hive, Power BI, and Docker. The pipeline fetches Covid data from GitHub and stages it in HDFS Data Warehouse, which is then transformed using PySpark. The transformed data is loaded into Hive for analytics, and the results are visualized using Power BI. The entire process is executed in Docker containers, ensuring consistency and reliability in the pipeline.The pipeline sends email notifications for success or failure and also posts messages to a Slack channel..</li>    
          
      </ul>
    </div>
    </section>
    
    <section id="opportunities" class="info-section">
        <div class="container">
            <h2>Open to Opportunities</h2>
            <p>I am currently seeking new challenges and opportunities in data engineering. If you have an opportunity or know someone who might, please feel free to <a href="mailto:teli.abhi01@gmail.com">contact me</a>.</p>
        </div>
    </section>
    
    <section id="resume" class="info-section">
        <div class="container">
            <h2>Resume</h2>
            <p>You can view my latest resume <a href="AbhiTeliResume.pdf" target="_blank">here</a>.</p>
        </div>
    </section>
    
    <footer>
        <p>&copy; 2024 Abhishek Teli | <a href="mailto:teli.abhi01@gmail.com" class="button">Contact Me</a></p>
    </footer>
</body>
</html>
